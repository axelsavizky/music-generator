from music21 import *
import os
import random
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
from sklearn.model_selection import train_test_split

from keras.layers import *
from keras.models import *
from keras.callbacks import *
import keras.backend as K
from keras.models import load_model

import sys

path = "gwern/midis"
frequent_notes_threshold = 256

n_of_timesteps = 32
evaluation_percentage = 0.2 # 20% of the data will be used as evaluation

output_dimension = 100
kernel_size = 3
epochs = 50

len_of_predictions = 30

def read_midi(file):
    
    print("Reading: " + file)
    
    notes=[]
    notes_to_parse = None
    
    #parsing a midi file
    try:
        midi = converter.parse(file)
    except:
        return np.array([])
  
    #grouping based on different instruments
    s2 = instrument.partitionByInstrument(midi)
    if not s2:
        return np.array([])
    #Looping over all the instruments
    for part in s2.parts:
    
        #select elements of only piano
        if 'Piano' in str(part): 
        
            notes_to_parse = part.recurse() 
      
            #finding whether a particular element is note or a chord
            for element in notes_to_parse:
                
                #note
                if isinstance(element, note.Note):
                    notes.append(str(element.pitch))
                
                #chord
                elif isinstance(element, chord.Chord):
                    notes.append('.'.join(str(n) for n in element.normalOrder))
    if len(notes) < 1:
        print(file+' not piano-endowed')
    return np.array(notes)

def convert_to_midi(prediction_output, filename):
   
    offset = 0
    output_notes = []

    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:
        
        # pattern is a chord
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                
                cn=int(current_note)
                new_note = note.Note(cn)
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
                
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
            
        # pattern is a note
        else:
            
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)

        # increase offset each iteration so that notes do not stack
        offset += 1
    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp=filename)
    
from glob import glob
from multiprocess import Pool
import time


TRAINING_SET_SIZE = int(sys.argv[1]) ## Cambiar
NTHREADS = 32 ## Cambiar si necesario

files = [y for x in os.walk(path) for y in glob(os.path.join(x[0], '*.mid'))]
print(len(files))
start = time.time()
#files=[i for i in os.listdir(path) if i.endswith(".mid")]
with Pool(NTHREADS) as p:
    notes_array = p.map(read_midi, files[:TRAINING_SET_SIZE])

print('filtering...')
notes_array = [e for e in notes_array if e.shape[0] > 2]

notes_array = np.array(notes_array, dtype=object)
print('notes...')
notes_ = [element for note_ in notes_array for element in note_]
end = time.time()

print(f'took {end - start} seconds')
print(notes_array.shape[0])

unique_notes = list(set(notes_))
print(len(unique_notes))

freq = dict(Counter(notes_))

no=[count for _,count in freq.items()]

frequent_notes = [note_ for note_, count in freq.items() if count>=frequent_notes_threshold]
print(f'keeping {frequent_notes} notes')

# Get the same dataset only with frequent notes
new_music=[]

for notes in notes_array:
    new_music.append([note for note in notes if note in frequent_notes])
    
new_music = np.array(new_music, dtype=object)


inputs = []
outputs = []

for notes_ in new_music:
    for i in range(0, len(notes_) - n_of_timesteps, 1):
        
        inputs.append(notes_[i:i + n_of_timesteps])
        outputs.append(notes_[i + n_of_timesteps])
        
inputs=np.array(inputs)
outputs=np.array(outputs)


unique_inputs = list(set(inputs.ravel()))
input_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_inputs))
import pickle
filehandler = open(sys.argv[2]+'_dict.pickle','wb')
pickle.dump(input_note_to_int, filehandler)
filehandler.close()


input_seq=[]
for input_ in inputs:
    input_seq.append([input_note_to_int[note_] for note_ in input_])
    
input_seq = np.array(input_seq)


unique_outputs = list(set(outputs))
output_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_outputs)) 
output_seq = np.array([output_note_to_int[note_] for note_ in outputs])


input_training, input_validation, output_training, output_validation = train_test_split(input_seq,output_seq,test_size=evaluation_percentage,random_state=0)


K.clear_session()
model = Sequential()

# Parameters explanation: https://keras.io/api/layers/core_layers/embedding/
model.add(Embedding(len(unique_inputs), output_dimension, input_length=n_of_timesteps,trainable=True)) 

# Parameters explanation: https://keras.io/api/layers/convolution_layers/convolution1d/
model.add(Conv1D(n_of_timesteps*2*2,kernel_size, padding='causal',activation='relu'))
model.add(Dropout(0.2))
model.add(MaxPool1D(2))
    
model.add(Conv1D(n_of_timesteps*4*2,kernel_size, activation='relu',dilation_rate=2,padding='causal'))
model.add(Dropout(0.2))
model.add(MaxPool1D(2))

model.add(Conv1D(n_of_timesteps*8*2,kernel_size, activation='relu',dilation_rate=4,padding='causal'))
model.add(Dropout(0.2))
model.add(MaxPool1D(2))
          
#model.add(Conv1D(256,5,activation='relu'))    
model.add(GlobalMaxPool1D())

# Parameters explanation: https://keras.io/api/layers/core_layers/dense/
# 256 -> 512
model.add(Dense(512, activation='relu'))
model.add(Dense(len(unique_outputs), activation='softmax'))
    
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')

model_name = sys.argv[2] ## Cambiar.

epochs = 50

checkpoint = ModelCheckpoint(model_name, monitor='val_loss', mode='min', save_best_only=True,verbose=1)
history = model.fit(np.array(input_training),np.array(output_training), batch_size=1024, epochs=epochs, validation_data=(np.array(input_validation),np.array(output_validation)),verbose=1, callbacks=[checkpoint])







