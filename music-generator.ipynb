{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/lstrika/anaconda2/bin/conda\", line 13, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Users/lstrika/anaconda2/lib/python2.7/site-packages/conda/cli/main.py\", line 149, in main\n",
      "    from ..exceptions import conda_exception_handler\n",
      "  File \"/Users/lstrika/anaconda2/lib/python2.7/site-packages/conda/exceptions.py\", line 19, in <module>\n",
      "    from ._vendor.auxlib.entity import EntityEncoder\n",
      "  File \"/Users/lstrika/anaconda2/lib/python2.7/site-packages/conda/_vendor/auxlib/entity.py\", line 253, in <module>\n",
      "    from .collection import AttrDict, frozendict, make_immutable\n",
      "  File \"/Users/lstrika/anaconda2/lib/python2.7/site-packages/conda/_vendor/auxlib/collection.py\", line 10, in <module>\n",
      "    from .compat import isiterable, iteritems, odict, text_type\n",
      "  File \"/Users/lstrika/anaconda2/lib/python2.7/site-packages/conda/_vendor/auxlib/compat.py\", line 10, in <module>\n",
      "    from ._vendor.five import WhateverIO as StringIO, with_metaclass\n",
      "  File \"/Users/lstrika/anaconda2/lib/python2.7/site-packages/conda/_vendor/auxlib/_vendor/five.py\", line 80, in <module>\n",
      "    absolute_to_nanoseconds = CoreServices.AbsoluteToNanoseconds\n",
      "  File \"/Users/lstrika/anaconda2/lib/python2.7/ctypes/__init__.py\", line 379, in __getattr__\n",
      "    func = self.__getitem__(name)\n",
      "  File \"/Users/lstrika/anaconda2/lib/python2.7/ctypes/__init__.py\", line 384, in __getitem__\n",
      "    func = self._FuncPtr((name_or_ordinal, self))\n",
      "AttributeError: dlsym(RTLD_DEFAULT, AbsoluteToNanoseconds): symbol not found\n",
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Processing /Users/lstrika/Library/Caches/pip/wheels/00/35/5f/6166b28b727e7e692690305417ee52707335c28041b48de397/music21-4.1.0-cp27-none-any.whl\n"
     ]
    }
   ],
   "source": [
    "!conda activate py37\n",
    "!pip install music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"gwern/midis\"\n",
    "frequent_notes_threshold = 50\n",
    "\n",
    "n_of_timesteps = 32\n",
    "evaluation_percentage = 0.2 # 20% of the data will be used as evaluation\n",
    "\n",
    "output_dimension = 100\n",
    "kernel_size = 3\n",
    "epochs = 50\n",
    "\n",
    "len_of_predictions = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aux functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_midi(file):\n",
    "    \n",
    "    print(\"Reading: \" + file)\n",
    "    \n",
    "    notes=[]\n",
    "    notes_to_parse = None\n",
    "    \n",
    "    #parsing a midi file\n",
    "    try:\n",
    "        midi = converter.parse(file)\n",
    "    except:\n",
    "        return np.array([])\n",
    "  \n",
    "    #grouping based on different instruments\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "    if not s2:\n",
    "        return np.array([])\n",
    "    #Looping over all the instruments\n",
    "    for part in s2.parts:\n",
    "    \n",
    "        #select elements of only piano\n",
    "        if 'Piano' in str(part): \n",
    "        \n",
    "            notes_to_parse = part.recurse() \n",
    "      \n",
    "            #finding whether a particular element is note or a chord\n",
    "            for element in notes_to_parse:\n",
    "                \n",
    "                #note\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                \n",
    "                #chord\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return np.array(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_midi(prediction_output, filename):\n",
    "   \n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        \n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                \n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        # pattern is a note\n",
    "        else:\n",
    "            \n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 1\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp=filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77152\n",
      "Reading: gwern/midis/lmd_matched/R/R/I/TRRRIVC12903CA6C5A/fa09fc4cc12b6f2d085ecb98650d2de6.midReading: gwern/midis/lmd_matched/R/R/I/TRRRION128F145EBB7/9048c4045d7d8b2837f3725d7995dc1c.midReading: gwern/midis/lmd_matched/R/R/I/TRRRILO128F422FFED/0ec315d7be357f130de5206e67c33b36.midReading: gwern/midis/lmd_matched/R/R/U/TRRRUFD12903CD7092/6ca2a1f03a193c336e18e86b5d845f34.midReading: gwern/midis/lmd_matched/R/R/I/TRRRIVC12903CA6C5A/ef6b163c2a5d18dbffa3b24aaf21419d.mid\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reading: gwern/midis/lmd_matched/R/R/I/TRRRIVC12903CA6C5A/6a4b5d29cecc86c2078f93edc8866299.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/music21/midi/translate.py:790: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent PROGRAM_CHANGE, track=0, channel=10, data=0>; getting generic UnpitchedPercussion\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: gwern/midis/lmd_matched/R/R/I/TRRRIVC12903CA6C5A/ee79e6b07b5bd944a4571918cdee715b.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/music21/midi/translate.py:790: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent PROGRAM_CHANGE, track=2, channel=10, data=0>; getting generic UnpitchedPercussion\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: gwern/midis/lmd_matched/R/R/I/TRRRIVC12903CA6C5A/9f9c81dd249087150c9eac829c0f7c89.mid\n",
      "Reading: gwern/midis/lmd_matched/R/R/N/TRRRNPV128F42AAA55/b9dc31daae7e35deffae077a705e01e7.mid\n",
      "Reading: gwern/midis/lmd_matched/R/R/N/TRRRNGS12903CD16D9/18e05979f4132614733c340185809dee.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/music21/midi/translate.py:790: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent PROGRAM_CHANGE, track=16, channel=10, data=0>; getting generic UnpitchedPercussion\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/music21/midi/translate.py:790: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent PROGRAM_CHANGE, track=17, channel=10, data=0>; getting generic UnpitchedPercussion\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/music21/midi/translate.py:790: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent PROGRAM_CHANGE, track=18, channel=10, data=0>; getting generic UnpitchedPercussion\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/music21/midi/translate.py:790: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent PROGRAM_CHANGE, track=19, channel=10, data=0>; getting generic UnpitchedPercussion\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/music21/midi/translate.py:790: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent PROGRAM_CHANGE, track=20, channel=10, data=0>; getting generic UnpitchedPercussion\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/music21/midi/translate.py:790: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent PROGRAM_CHANGE, track=21, channel=10, data=0>; getting generic UnpitchedPercussion\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/music21/midi/translate.py:790: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent PROGRAM_CHANGE, track=22, channel=10, data=0>; getting generic UnpitchedPercussion\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/music21/midi/translate.py:790: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent PROGRAM_CHANGE, track=23, channel=10, data=0>; getting generic UnpitchedPercussion\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/music21/midi/translate.py:790: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent PROGRAM_CHANGE, track=24, channel=10, data=0>; getting generic UnpitchedPercussion\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering...\n",
      "notes...\n",
      "took 39.8263201713562 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/dzytbyv51dd88zf00d9qj1100000gn/T/ipykernel_6522/2862025795.py:17: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  notes_array = [e for e in notes_array if e != np.array([])]\n",
      "/var/folders/xy/dzytbyv51dd88zf00d9qj1100000gn/T/ipykernel_6522/2862025795.py:17: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  notes_array = [e for e in notes_array if e != np.array([])]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from multiprocess import Pool\n",
    "import time\n",
    "\n",
    "\n",
    "TRAINING_SET_SIZE = 1024\n",
    "NTHREADS = 32\n",
    "\n",
    "files = [y for x in os.walk(path) for y in glob(os.path.join(x[0], '*.mid'))]\n",
    "print(len(files))\n",
    "start = time.time()\n",
    "#files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "with Pool(NTHREADS) as p:\n",
    "    notes_array = p.map(read_midi, files[:TRAINING_SET_SIZE])\n",
    "\n",
    "print('filtering...')\n",
    "notes_array = [e for e in notes_array if e != np.array([])]\n",
    "\n",
    "notes_array = np.array(notes_array, dtype=object)\n",
    "print('notes...')\n",
    "notes_ = [element for note_ in notes_array for element in note_]\n",
    "end = time.time()\n",
    "\n",
    "print(f'took {end - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "485.8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([t.shape for t in notes_array]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    }
   ],
   "source": [
    "unique_notes = list(set(notes_))\n",
    "print(len(unique_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([101.,  20.,  12.,   7.,   1.,   2.,   0.,   1.,   0.,   1.]),\n",
       " array([  1. ,  16.9,  32.8,  48.7,  64.6,  80.5,  96.4, 112.3, 128.2,\n",
       "        144.1, 160. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 302,
       "width": 319
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Notes frequency\n",
    "freq = dict(Counter(notes_))\n",
    "\n",
    "no=[count for _,count in freq.items()]\n",
    "\n",
    "#set the figure size\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#plot\n",
    "plt.hist(no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_notes = [note_ for note_, count in freq.items() if count>=frequent_notes_threshold]\n",
    "\n",
    "# Get the same dataset only with frequent notes\n",
    "new_music=[]\n",
    "\n",
    "for notes in notes_array:\n",
    "    new_music.append([note for note in notes if note in frequent_notes])\n",
    "    \n",
    "new_music = np.array(new_music, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for notes_ in new_music:\n",
    "    for i in range(0, len(notes_) - n_of_timesteps, 1):\n",
    "        \n",
    "        inputs.append(notes_[i:i + n_of_timesteps])\n",
    "        outputs.append(notes_[i + n_of_timesteps])\n",
    "        \n",
    "inputs=np.array(inputs)\n",
    "outputs=np.array(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_inputs = list(set(inputs.ravel()))\n",
    "input_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_inputs))\n",
    "\n",
    "input_seq=[]\n",
    "for input_ in inputs:\n",
    "    input_seq.append([input_note_to_int[note_] for note_ in input_])\n",
    "    \n",
    "input_seq = np.array(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_outputs = list(set(outputs))\n",
    "output_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_outputs)) \n",
    "output_seq = np.array([output_note_to_int[note_] for note_ in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_training, input_validation, output_training, output_validation = train_test_split(input_seq,output_seq,test_size=evaluation_percentage,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM architecture. We will use WaveNet for now\n",
    "# def lstm():\n",
    "#   model = Sequential()\n",
    "#   model.add(LSTM(128,return_sequences=True))\n",
    "#   model.add(LSTM(128))\n",
    "#   model.add(Dense(256))\n",
    "#   model.add(Activation('relu'))\n",
    "#   model.add(Dense(n_vocab))\n",
    "#   model.add(Activation('softmax'))\n",
    "#   model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "#   return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 10:05:49.270015: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 32, 100)           1200      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 32, 128)           38528     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16, 256)           98560     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 512)            393728    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 512)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 512)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                6156      \n",
      "=================================================================\n",
      "Total params: 800,828\n",
      "Trainable params: 800,828\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = Sequential()\n",
    "\n",
    "# Parameters explanation: https://keras.io/api/layers/core_layers/embedding/\n",
    "model.add(Embedding(len(unique_inputs), output_dimension, input_length=n_of_timesteps,trainable=True)) \n",
    "\n",
    "# Parameters explanation: https://keras.io/api/layers/convolution_layers/convolution1d/\n",
    "model.add(Conv1D(n_of_timesteps*2*2,kernel_size, padding='causal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "    \n",
    "model.add(Conv1D(n_of_timesteps*4*2,kernel_size, activation='relu',dilation_rate=2,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(n_of_timesteps*8*2,kernel_size, activation='relu',dilation_rate=4,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "          \n",
    "#model.add(Conv1D(256,5,activation='relu'))    \n",
    "model.add(GlobalMaxPool1D())\n",
    "\n",
    "# Parameters explanation: https://keras.io/api/layers/core_layers/dense/\n",
    "# 256 -> 512\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(len(unique_outputs), activation='softmax'))\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.4147 - val_loss: 2.4110\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.41103, saving model to Gwern_big_model_1024.h5\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.3755 - val_loss: 2.3860\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.41103 to 2.38602, saving model to Gwern_big_model_1024.h5\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.3705 - val_loss: 2.3644\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.38602 to 2.36444, saving model to Gwern_big_model_1024.h5\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.3322 - val_loss: 2.3399\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.36444 to 2.33991, saving model to Gwern_big_model_1024.h5\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.2828 - val_loss: 2.3106\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.33991 to 2.31060, saving model to Gwern_big_model_1024.h5\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.2412 - val_loss: 2.2737\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.31060 to 2.27374, saving model to Gwern_big_model_1024.h5\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.1996 - val_loss: 2.2320\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.27374 to 2.23200, saving model to Gwern_big_model_1024.h5\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.1666 - val_loss: 2.1990\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.23200 to 2.19896, saving model to Gwern_big_model_1024.h5\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.1398 - val_loss: 2.1804\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.19896 to 2.18037, saving model to Gwern_big_model_1024.h5\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.1211 - val_loss: 2.1674\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.18037 to 2.16739, saving model to Gwern_big_model_1024.h5\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Gwern_big_model_1024.h5'\n",
    "epochs = 50\n",
    "checkpoint = ModelCheckpoint(model_name, monitor='val_loss', mode='min', save_best_only=True,verbose=1)\n",
    "history = model.fit(np.array(input_training),np.array(output_training), batch_size=1024, epochs=epochs, validation_data=(np.array(input_validation),np.array(output_validation)),verbose=1, callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('Gwern_big_model_1024.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate music!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "ind = np.random.randint(0,len(input_validation)-1)\n",
    "print(ind)\n",
    "random_music = input_validation[ind]\n",
    "        \n",
    "predictions=[]\n",
    "for i in range(len_of_predictions):\n",
    "\n",
    "    random_music = random_music.reshape(1,n_of_timesteps)\n",
    "\n",
    "    prob   = model.predict(random_music)[0]\n",
    "    \n",
    "    output_pred = np.argmax(prob, axis=0)\n",
    "    print(output_pred)\n",
    "    predictions.append(output_pred)\n",
    "\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),output_pred)\n",
    "    random_music = random_music[1:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4', 'A4']\n"
     ]
    }
   ],
   "source": [
    "input_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_inputs)) \n",
    "predicted_notes = [input_int_to_note[i] for i in predictions]\n",
    "print(predicted_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_midi(predicted_notes, 'gwern_music_small_1024_3.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def beam_search_generator(starting_data, model, len_of_songs, k):\n",
    "    n_of_timesteps = len(starting_data[0])\n",
    "    \n",
    "    # starting k randomly generated states\n",
    "    indices = np.random.randint(len(starting_data), size=k)\n",
    "    # [prediction, random_music, score]\n",
    "    sequences = [[[], starting_data[index], 0.0] for index in indices]\n",
    "    # walk over each step in sequence\n",
    "\n",
    "    for _ in range(len_of_songs):\n",
    "        assert(len(sequences) == k)\n",
    "        all_candidates = list()\n",
    "        all_random_music = [sequence[1] for sequence in sequences]\n",
    "        all_random_music = np.array(all_random_music).reshape(k, n_of_timesteps)\n",
    "        prob = model.predict(all_random_music)\n",
    "        # expand each current candidate\n",
    "        for i in range(k):\n",
    "            seq, random_music, score = sequences[i]\n",
    "            for j in range(len(prob[i])):\n",
    "                assert(len(random_music) == n_of_timesteps)\n",
    "                new_random_music = np.insert(random_music,len(random_music),j)\n",
    "                new_random_music = new_random_music[1:]\n",
    "                candidate = [seq + [j], new_random_music, score - math.log(prob[i][j])]\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[2])\n",
    "        # select k best\n",
    "        sequences = ordered[:k]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], array([2, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 41.43330583838211]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], array([2, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 41.56365799877024]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], array([2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 41.75512872036035]\n"
     ]
    }
   ],
   "source": [
    "# generate music with beam search\n",
    "result = beam_search_generator(input_validation, model, len_of_predictions, 3)\n",
    "# print generated songs\n",
    "for seq in result:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3']\n"
     ]
    }
   ],
   "source": [
    "# print best song generated and save midi\n",
    "predicted_notes = [input_int_to_note[i] for i in seq[0]]\n",
    "print(predicted_notes)\n",
    "convert_to_midi(predicted_notes, 'gwern_music_small_1024_beam_search.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "\n",
    "def get_predictions(model, song):\n",
    "    N = len(song)\n",
    "    predictions = []\n",
    "    for i in range(1, N):\n",
    "        # Get the first i notes\n",
    "        partial_song = song[:i]\n",
    "        # Pad it with zeroes to get 32 notes\n",
    "        partial_song_with_padding = np.pad(partial_song, (n_of_timesteps - len(partial_song), 0), 'constant', constant_values=0)\n",
    "        # Evaluate it\n",
    "        prob = model.predict(partial_song_with_padding.reshape(1, 32))\n",
    "        predictions.append(prob[0][-1])\n",
    "    return predictions\n",
    "\n",
    "def stable_perplexity(predictions):\n",
    "    N = len(predictions)\n",
    "    log_perplexity = 0\n",
    "    for prob in predictions:\n",
    "        log_perplexity += np.log(prob)\n",
    "    return np.exp(-log_perplexity/float(N))\n",
    "\n",
    "def default_perplexity(predictions):\n",
    "    N = len(predictions)\n",
    "    perplexity = 1\n",
    "    for prob in predictions:\n",
    "        perplexity*=prob\n",
    "    return (1/perplexity)**(1/float(N))\n",
    "\n",
    "def calculate_perplexity(model, song):\n",
    "    return default_perplexity(get_predictions(model, song))\n",
    "\n",
    "def calculate_perplexity_stable(model, song):    \n",
    "    return stable_perplexity(get_predictions(model, song))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6880580828615903\n",
      "2.6880580828615908\n"
     ]
    }
   ],
   "source": [
    "### Test perplexities\n",
    "probs = np.random.random(500)\n",
    "print(stable_perplexity(probs))\n",
    "print(default_perplexity(probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6449214695973116e-33\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity with random song\n",
    "index = np.random.randint(len(input_validation))\n",
    "\n",
    "print(calculate_perplexity(model, input_validation[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py37 (textGenRnn)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
